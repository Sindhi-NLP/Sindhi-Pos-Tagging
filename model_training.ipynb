{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc6ae05a-276e-4857-be2c-64a5f2a31ac9",
   "metadata": {},
   "source": [
    "# Part-of-Speech Tagging for Sindhi Language\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of training and evaluating a Part-of-Speech (POS) tagger for the Sindhi language using Conditional Random Fields (CRF).\n",
    "\n",
    "Sindhi is an Indo-Aryan language spoken primarily in the Sindh province of Pakistan and parts of India. It has a rich morphological structure and a unique script, which presents interesting challenges for natural language processing tasks.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Preprocess and prepare Sindhi language data for POS tagging\n",
    "2. Extract relevant features from Sindhi text\n",
    "3. Train a CRF model for POS tagging\n",
    "4. Evaluate the model's performance\n",
    "5. Demonstrate inference on new Sindhi sentences\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We use a custom dataset of Sindhi text, manually annotated with POS tags. The data is stored in a CSV file named 'output.csv', containing tokenized Sindhi text with corresponding linguistic features and POS tags.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "We employ Conditional Random Fields (CRF), a statistical modeling method often used for structured prediction tasks like POS tagging. CRFs are particularly suitable for Sindhi due to their ability to capture context and handle rich feature sets.\n",
    "\n",
    "## Libraries Used\n",
    "\n",
    "- pandas: For data manipulation and analysis\n",
    "- pycrfsuite: For implementing Conditional Random Fields\n",
    "- scikit-learn: For evaluation metrics (optional, if used)\n",
    "\n",
    "## Note\n",
    "\n",
    "POS tagging for Sindhi presents unique challenges due to the language's morphological complexity and the limited availability of large, annotated datasets. This project aims to contribute to the growing field of NLP for less-resourced languages.\n",
    "\n",
    "Let's begin by importing necessary libraries and loading our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64c7740b-198a-4da4-a817-89153554310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pycrfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd46a48-26d4-442f-977f-2526a592d319",
   "metadata": {},
   "source": [
    "## Load and Examine Data\r\n",
    "\r\n",
    "In this section, we load the Sindhi POS dataset from a CSV file and perform an initial examination to understand its structure and content. The dataset contains information for part-of-speech tagging and includes various features such as `FORM`, `LEMMA`, `UPOS`, `XPOS`, `FEATS`, and `Pronunciationa.describe()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cb3c6d1-764c-42fc-80d3-b0bf6ebabfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FORM</th>\n",
       "      <th>LEMMA</th>\n",
       "      <th>UPOS</th>\n",
       "      <th>XPOS</th>\n",
       "      <th>FEATS</th>\n",
       "      <th>Pronunciation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>يقين</td>\n",
       "      <td>يقين</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>XPOS =</td>\n",
       "      <td>yaqeen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ڪرڻ</td>\n",
       "      <td>ڪر</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VB</td>\n",
       "      <td>XPOS =</td>\n",
       "      <td>karan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>سان</td>\n",
       "      <td>سان</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>XPOS =</td>\n",
       "      <td>Saan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>اڪثر</td>\n",
       "      <td>اڪثر</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td>XPOS =</td>\n",
       "      <td>Aksar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ڌوڪو</td>\n",
       "      <td>ڌوڪو</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td>XPOS =</td>\n",
       "      <td>dhoko</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  FORM LEMMA  UPOS XPOS   FEATS Pronunciation\n",
       "0   1  يقين  يقين  NOUN   NN  XPOS =        yaqeen\n",
       "1   2   ڪرڻ    ڪر  VERB   VB  XPOS =         karan\n",
       "2   3   سان   سان   ADP   IN  XPOS =          Saan\n",
       "3   4  اڪثر  اڪثر   ADV   RB  XPOS =         Aksar\n",
       "4   5  ڌوڪو  ڌوڪو  NOUN   NN  XPOS =         dhoko"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('SindhiPosDataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392cb40-61b0-432c-9290-f6393f97feed",
   "metadata": {},
   "source": [
    "### Feature Extraction Function\n",
    "\n",
    "This cell defines the function `extract_features` used for extracting features from tokens in a sequence, which is essential for tasks such as Part-of-Speech (POS) tagging. The function creates features for a given token based on its position within the sequence.\n",
    "\n",
    "#### Function Definition: `extract_features(tokens, idx)`\n",
    "\n",
    "- **Parameters**:\n",
    "  - `tokens`: A list of dictionaries where each dictionary represents a token with its attributes.\n",
    "  - `idx`: The index of the current token for which features are being extracted.\n",
    "\n",
    "- **Returns**:\n",
    "  - A dictionary of features for the token at the specified index.\n",
    "\n",
    "#### Features Extracted:\n",
    "- **Current Word**: The word of the token at the given index.\n",
    "- **Previous Word**: The word of the token preceding the current token, if not at the start of the sequence.\n",
    "- **Next Word**: The word of the token following the current token, if not at the end of the sequence.\n",
    "- **Beginning of Sentence (BOS)**: A flag indicating the start of the sequence.\n",
    "- **End of Sentence (EOS)**: A flag indicating the end of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72765bbe-b817-4acd-8eb1-cd83bd439200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract features from a token\n",
    "def extract_features(tokens, idx):\n",
    "    token = tokens[idx]\n",
    "    features = {\n",
    "        'word': token['FORM'],\n",
    "    }\n",
    "    \n",
    "    if idx > 0:\n",
    "        prev_token = tokens[idx - 1]\n",
    "        features.update({\n",
    "            '-1:word': prev_token['FORM'],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "\n",
    "    if idx < len(tokens) - 1:\n",
    "        next_token = tokens[idx + 1]\n",
    "        features.update({\n",
    "            '+1:word': next_token['FORM'],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e2a63-0438-436b-901f-a18b565006f7",
   "metadata": {},
   "source": [
    "### Group Data into Sentences\r\n",
    "\r\n",
    "This cell processes the dataset to group tokens into their respective sentences. The dataset is assumed to be in a format where each row represents a token with an associated sentence ID. The code iterates through the dataset, collecting tokens into sentences based on their IDs.\r\n",
    "\r\n",
    "#### Code Explanation\r\n",
    "\r\n",
    "- **Initialization**:\r\n",
    "  - `sentences`: An empty list to store the sentences after grouping.\r\n",
    "  - `current_sentence`: An empty list to collect tokens for the current sentence being processed.\r\n",
    "\r\n",
    "- **Processing**:\r\n",
    "  - The code iterates through each row of the dataset.\r\n",
    "  - For each row, it checks if the sentence ID (`row['ID']`) indicates the start of a new sentence (`ID == 1`) and if there are tokens in `current_sentence`. If true, it appends `current_sentence` to `sentences` and resets `current_sentence` for the new sentence.\r\n",
    "  - The current token is then added to `current_sentence`.\r\n",
    "\r\n",
    "- **Final Check**:\r\n",
    "  - After the loop, if there are remaining tokens in `current_sentence`, they are added to `sentences`.\r\n",
    "\r\n",
    "#### Result\r\n",
    "\r\n",
    "- `sentences`: A list of lists, where each inner list contains tokens representin.iterrows():\r\n",
    "    if row['ID']\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "278a8f23-58c9-426c-ac9c-c037ea780851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data into sentences\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    if row['ID'] == 1 and current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        current_sentence = []\n",
    "    current_sentence.append(row)\n",
    "\n",
    "if current_sentence:\n",
    "    sentences.append(current_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4f0bd-9f6d-4e0d-9c3b-a340dfe70fe6",
   "metadata": {},
   "source": [
    "### Extract Features and Labels\n",
    "\n",
    "In this cell, we prepare the dataset for model training by extracting features and labels from the preprocessed sentences. This step is crucial for training a sequence labeling model, such as a Conditional Random Field (CRF) model, for part-of-speech (POS) tagging.\n",
    "\n",
    "#### Code Explanation\n",
    "\n",
    "- **Initialization**:\n",
    "  - `X`: An empty list to hold the feature sets for each token in each sentence.\n",
    "  - `y`: An empty list to hold the POS tags for each token in each sentence.\n",
    "\n",
    "- **Processing**:\n",
    "  - **Iteration Over Sentences**:\n",
    "    - For each sentence in the `sentences` list:\n",
    "      - **Feature Extraction**:\n",
    "        - `extract_features(sentence, idx)`: The `extract_features` function is called for each token in the sentence. It generates a feature dictionary based on the current token and its neighboring tokens (previous and next).\n",
    "        - This produces a list of feature dictionaries, where each dictionary corresponds to a token's features.\n",
    "      - **Label Extraction**:\n",
    "        - `[token['UPOS'] for token in sentence]`: Extracts the Universal Part-Of-Speech (UPOS) tag for each token in the sentence.\n",
    "\n",
    "- **Result**:\n",
    "  - `X`: A list of feature lists, where each sublist corresponds to a sentence and contains dictionaries of token features.\n",
    "  - `y`: A list of label lists, where each sublist corresponds to a sentence and contains the UPOS tags for the tokens in that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c2456cbc-84ab-4a8b-89ee-39d5b3831356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    X.append([extract_features(sentence, idx) for idx in range(len(sentence))])\n",
    "    y.append([token['UPOS'] for token in sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320bbdf-cd4a-4f10-a9c3-3b55363c3499",
   "metadata": {},
   "source": [
    "This Python code defines a function `convert_features` to transform feature data into a format suitable for the PyCRFSuite library. PyCRFSuite expects features in a specific dictionary-based format for training and prediction.\n",
    "\n",
    "1. **Function Definition:**\n",
    "   * `def convert_features(X):` defines a function named `convert_features` that takes a list of feature dictionaries `X` as input.\n",
    "\n",
    "2. **Feature Conversion:**\n",
    "   * `[{k: str(v) for k, v in x.items()} for x in X]` is a list comprehension that iterates over each feature dictionary `x` in the input list `X`.\n",
    "   * For each feature dictionary `x`, it creates a new dictionary where the keys and values are converted to strings using `str(v)`. This is necessary because PyCRFSuite requires string-based features.\n",
    "   * The resulting list of converted feature dictionaries is returned.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "The function returns a list of feature dictionaries, where each dictionary represents a data point and its features are converted to strings. This format is compatible with PyCRFSuite's input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f4e1a8c-7194-47a8-9869-dc287ca070b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features to the format required by pycrfsuite\n",
    "def convert_features(X):\n",
    "    return [{k: str(v) for k, v in x.items()} for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c711a-124e-4e17-a865-a64303ac78b4",
   "metadata": {},
   "source": [
    "This Python code snippet demonstrates the basic structure for training a Conditional Random Field (CRF) model using the PyCRFSuite library. It involves creating a trainer instance, appending training data, and implicitly performing the training process.\n",
    "\n",
    "1. **Import:**\n",
    "   * `pycrfsuite.Trainer` is imported to create a CRF trainer object.\n",
    "\n",
    "2. **Trainer Initialization:**\n",
    "   * `trainer = pycrfsuite.Trainer(verbose=False)` creates a CRF trainer instance. The `verbose=False` argument suppresses training progress messages.\n",
    "\n",
    "3. **Data Appending:**\n",
    "   * The `for` loop iterates over pairs of feature sequences (`xseq`) and label sequences (`yseq`).\n",
    "   * `trainer.append(convert_features(xseq), yseq)` adds each feature-label pair to the trainer's dataset. The `convert_features` function (assumed to be defined elsewhere) transforms feature sequences into a format suitable for PyCRFSuite.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* The `Trainer` object accumulates training data in memory.\n",
    "* The `convert_features` function is essential for preparing feature data in the correct format.\n",
    "* The code snippet focuses on data preparation and appending, omitting the actual model training and saving steps.\n",
    "\n",
    "**Additional Considerations:**\n",
    "\n",
    "* **Model Training:** After appending all training data, you would typically call the `trainer.train(model_file)` method to train the CRF model and save it to a file.\n",
    "* **Feature Engineering:** The quality of the extracted features significantly impacts the model's performance.\n",
    "* **Hyperparameter Tuning:** PyCRFSuite offers various parameters to control the training process, which can be fine-tuned for optimal results.\n",
    "* **Evaluation:** Once the model is trained, it can be used to make predictions on new data and evaluated using appropriate metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc611827-f411-4f05-bacd-704bb82f0b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X, y):\n",
    "    trainer.append(convert_features(xseq), yseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76bee68-c6ed-4618-bcf3-b4f2e3de3aba",
   "metadata": {},
   "source": [
    "\n",
    "This Python code snippet demonstrates how to set hyperparameters for a CRF model using the PyCRFSuite library before training.\n",
    "\n",
    "1. **Import:**\n",
    "   - `pycrfsuite` is imported to access the `Trainer` class.\n",
    "\n",
    "2. **Trainer Initialization:**\n",
    "   * A `Trainer` object is created without specifying parameters initially.\n",
    "\n",
    "3. **Setting Parameters:**\n",
    "   * `trainer.set_params()` is used to configure the CRF model's hyperparameters:\n",
    "     - `c1`: L1 regularization parameter (coefficient)\n",
    "     - `c2`: L2 regularization parameter (coefficient)\n",
    "     - `max_iterations`: Maximum number of iterations for training\n",
    "     - `feature.possible_transitions`: Enables the inclusion of transition features in the model\n",
    "\n",
    "4. **Training:**\n",
    "   * `trainer.train('model.crfsuite')` initiates the model training process using the specified parameters and saves the trained model to 'model.crfsuite'.\n",
    "\n",
    "**Explanation of Parameters:**\n",
    "\n",
    "* **c1 (L1 penalty):** Controls the L1 regularization strength. Higher values induce sparsity in the model, potentially leading to feature selection.\n",
    "* **c2 (L2 penalty):** Controls the L2 regularization strength. It helps prevent overfitting by discouraging large weights.\n",
    "* **max_iterations:** Specifies the maximum number of iterations for the training algorithm.\n",
    "* **feature.possible_transitions:** Enables the inclusion of transition features, which capture dependencies between consecutive labels.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* Proper hyperparameter tuning is crucial for CRF model performance.\n",
    "* Experimentation with different parameter values is often necessary to find the optimal configuration.\n",
    "* Other hyperparameters might be available depending on the chosen training algorithm.\n",
    "\n",
    "**Additional Considerations:**\n",
    "\n",
    "* Consider using cross-validation or grid search to find the best hyperparameter combination.\n",
    "* Explore other regularization techniques or optimization algorithms offered by PyCRFSuite.\n",
    "* Evaluate the trained model's performance using appropriate metrics.\n",
    "\n",
    "By understanding these parameters and their impact, you can effectively fine-tune your CRF model for better performance on your specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ac78d26-289d-4312-bd7c-32c8173225a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 1.0,  # Coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # Coefficient for L2 penalty\n",
    "    'max_iterations': 50,  # Maximum number of iterations\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "# Train the model\n",
    "trainer.train('sindhiposmodel.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad979c8-b7e6-49b9-927c-e2c21abe5ca1",
   "metadata": {},
   "source": [
    "This Python code snippet demonstrates how to load a pre-trained CRF model using the `pycrfsuite.Tagger` class.\n",
    "\n",
    "\n",
    "1. **Import:**\n",
    "   - `pycrfsuite.Tagger` is imported to create a tagger object for prediction.\n",
    "\n",
    "2. **Tagger Creation:**\n",
    "   - `tagger = pycrfsuite.Tagger()` creates a tagger object.\n",
    "\n",
    "3. **Model Loading:**\n",
    "   - `tagger.open('model.crfsuite')` loads the pre-trained CRF model from the file 'model.crfsuite' into the tagger object.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* The loaded model can be used to make predictions on new data.\n",
    "* The model file format is specific to PyCRFSuite and might not be compatible with other libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "704d05bb-d347-4008-b382-812beea739c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.closing at 0x226986a2cf0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('sindhiposmodel.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa22b5-66f9-4b5c-9bc3-911d469948e6",
   "metadata": {},
   "source": [
    "This Python code defines a function `evaluate_model` to assess the performance of a CRF model on a given dataset.\n",
    "\n",
    "1. **Function Definition:**\n",
    "   * `def evaluate_model(X, y, tagger):` defines a function named `evaluate_model` that takes three arguments:\n",
    "     - `X`: A list of feature sequences.\n",
    "     - `y`: A list of corresponding label sequences.\n",
    "     - `tagger`: A trained CRF model instance.\n",
    "\n",
    "2. **Prediction:**\n",
    "   * `y_pred = []` initializes an empty list to store predicted label sequences.\n",
    "   * The `for` loop iterates over each feature sequence `xseq` in `X`.\n",
    "   * For each `xseq`, the `tagger.tag(convert_features(xseq))` method is used to obtain the predicted label sequence, which is appended to `y_pred`.\n",
    "\n",
    "3. **Accuracy Calculation:**\n",
    "   * `correct` and `total` are initialized to 0 to keep track of correct and total predictions.\n",
    "   * The outer `for` loop iterates over pairs of true label sequences (`yseq`) and predicted label sequences (`yseq_pred`).\n",
    "   * The inner `for` loop compares each predicted label (`y_hat`) with the corresponding true label (`y_true`).\n",
    "   * If the predicted label matches the true label, `correct` is incremented.\n",
    "   * `total` is incremented for each label comparison.\n",
    "\n",
    "4. **Accuracy Calculation:**\n",
    "   * The final accuracy is calculated by dividing the number of correct predictions (`correct`) by the total number of predictions (`total`).\n",
    "\n",
    "5. **Return Value:**\n",
    "   * The function returns the calculated accuracy as a floating-point value.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* The function assumes the availability of the `convert_features` function for feature conversion.\n",
    "* The accuracy metric used is simple accuracy, which might not be suitable for imbalanced datasets or other evaluation scenarios.\n",
    "* More sophisticated evaluation metrics like precision, recall, F1-score, or confusion matrices can be incorporated for a more comprehensive evaluation.\n",
    "\n",
    "**Improvements:**\n",
    "\n",
    "* Consider using built-in evaluation functions from PyCRFSuite or other libraries for efficiency and potential additional metrics.\n",
    "* Implement more robust evaluation metrics like precision, recall, and F1-score to provide a more comprehensive assessment of the model's performance.\n",
    "* Handle potential errors or exceptions that might occur during prediction.\n",
    "\n",
    "By understanding this code and incorporating potential improvements, you can effectively evaluate the performance of your CRF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9d7f3d8-71d4-41db-9c35-e7838a8ff52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model\n",
    "def evaluate_model(X, y, tagger):\n",
    "    y_pred = []\n",
    "    for xseq in X:\n",
    "        y_pred.append(tagger.tag(convert_features(xseq)))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for yseq, yseq_pred in zip(y, y_pred):\n",
    "        for y_true, y_hat in zip(yseq, yseq_pred):\n",
    "            if y_true == y_hat:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c3ce6-d699-4928-81fa-28832d5a9f9f",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to evaluate a CRF model and print the accuracy score.\n",
    "\n",
    "1. **Model Evaluation:**\n",
    "   * `accuracy = evaluate_model(X, y, tagger)` calls the `evaluate_model` function (defined previously) to calculate the accuracy of the CRF model on the given test data `X` and `y`, using the trained `tagger`.\n",
    "\n",
    "2. **Printing Accuracy:**\n",
    "   * `print(f'Accuracy: {accuracy:.4f}')` prints the calculated accuracy with four decimal places.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* The `evaluate_model` function is assumed to be defined as in the previous code snippet.\n",
    "* The code focuses on the final step of evaluating the model's performance.\n",
    "* The formatted printing provides a clear and readable output.\n",
    "\n",
    "**Additional Considerations:**\n",
    "\n",
    "* Consider using other evaluation metrics beyond accuracy, such as precision, recall, and F1-score.\n",
    "* For more complex evaluation scenarios, you might need to calculate these metrics manually or use specialized evaluation libraries.\n",
    "\n",
    "By combining this code with the previously provided `evaluate_model` function, you can effectively assess the performance of your CRF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc93a14d-94fc-403d-81e1-30e456fa4e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9645\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = evaluate_model(X, y, tagger)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e7f02-a06f-45b6-aa40-b85274402c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
